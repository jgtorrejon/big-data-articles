{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8f5e89",
   "metadata": {},
   "source": [
    "# Autoencoders Variacionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361ec4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIO DE LA DEMOSTRACIÓN DE UN PASO DE ENTRENAMIENTO ---\n",
      "Dato de entrada X (un '7' de 35 píxeles) definido.\n",
      "\n",
      "--- [Paso 1] Generando Predicciones ---\n",
      "1a. Encoder produce parámetros de la 'nube':\n",
      "    - Media (mu): [-0.15  0.02]\n",
      "    - Log-Varianza (log_var): [-0.11  0.16]\n",
      "\n",
      "1b. Se muestrea un vector latente 'z' de la nube: [ 0.77 -0.24]\n",
      "\n",
      "1c. Decoder genera la reconstrucción X_hat a partir de 'z'.\n",
      "    (Mostrando los primeros 10 de 35 píxeles de X_hat: [0.51 0.59 0.56 0.47 0.54 0.48 0.41 0.58 0.47 0.5 ])\n",
      "\n",
      "--- [Paso 2] Calculando la Pérdida Dual ---\n",
      "2a. Pérdida de Reconstrucción (qué tan diferente es X de X_hat): 23.67\n",
      "2b. Pérdida de Regularización KL (qué tan 'desorganizada' está la nube): 0.02\n",
      "--------------------------------------------------\n",
      "PÉRDIDA TOTAL (Suma de ambas): 23.69\n",
      "\n",
      "--- [Paso 3] Ajustando Parámetros del Modelo ---\n",
      "Peso de ejemplo ANTES de la actualización: -0.2202\n",
      "Peso de ejemplo DESPUÉS de la actualización: -0.2202\n",
      "¡El peso ha cambiado! El modelo está aprendiendo.\n",
      "\n",
      "--- [Paso 4] Repitiendo el Proceso ---\n",
      "Ahora, el ciclo completo se repetiría con nuevos datos (u los mismos).\n",
      "Veamos cómo cambia la pérdida si repetimos el proceso 5 veces más con el mismo dato:\n",
      "  Iteración 1: Pérdida Total = 23.61 (Recon: 23.57, KL: 0.03)\n",
      "  Iteración 2: Pérdida Total = 22.70 (Recon: 22.64, KL: 0.06)\n",
      "  Iteración 3: Pérdida Total = 22.12 (Recon: 22.02, KL: 0.10)\n",
      "  Iteración 4: Pérdida Total = 21.35 (Recon: 21.21, KL: 0.13)\n",
      "  Iteración 5: Pérdida Total = 18.58 (Recon: 18.45, KL: 0.12)\n",
      "\n",
      "Como se puede ver, la pérdida total tiende a disminuir a medida que el modelo se ajusta.\n",
      "--- FIN DE LA DEMOSTRACIÓN ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Defino una versión muy simple de un VAE para que podamos ver sus componentes.\n",
    "class ToyVAE(nn.Module):\n",
    "    def __init__(self, input_dim=35, latent_dim=2):\n",
    "        super(ToyVAE, self).__init__()\n",
    "        # Mi encoder tiene una capa que va a los parámetros de la distribución.\n",
    "        self.fc_encode = nn.Linear(input_dim, 20)\n",
    "        self.fc_mu = nn.Linear(20, latent_dim)      # Capa para la media\n",
    "        self.fc_log_var = nn.Linear(20, latent_dim) # Capa para la log-varianza\n",
    "\n",
    "        # Mi decoder tiene una capa para volver al espacio original.\n",
    "        self.fc_decode = nn.Linear(latent_dim, 20)\n",
    "        self.fc_output = nn.Linear(20, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc_encode(x))\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc_decode(z))\n",
    "        return torch.sigmoid(self.fc_output(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "#  la función de pérdida del VAE.\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE, KLD\n",
    "\n",
    "#  instancia de mi modelo y un optimizador.\n",
    "model = ToyVAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Creo mi único dato de entrada: una imagen de un \"7\" en formato 7x5 (35 píxeles).\n",
    "# El 1.0 representa un píxel encendido, el 0.0 uno apagado.\n",
    "X = torch.tensor([\n",
    "    1.0, 1.0, 1.0, 1.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 0.0,\n",
    "    0.0, 0.0, 1.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 0.0, 0.0, 0.0\n",
    "])\n",
    "\n",
    "print(\"--- INICIO DE LA DEMOSTRACIÓN DE UN PASO DE ENTRENAMIENTO ---\")\n",
    "print(f\"Dato de entrada X (un '7' de 35 píxeles) definido.\\n\")\n",
    "\n",
    "# --- PASO 1: Generar Predicciones (Forward Pass) ---\n",
    "print(\"--- [Paso 1] Generando Predicciones ---\")\n",
    "# Hago pasar mi dato 'X' a través del modelo.\n",
    "recon_X_hat, mu, log_var = model(X)\n",
    "\n",
    "print(f\"1a. Encoder produce parámetros de la 'nube':\")\n",
    "print(f\"    - Media (mu): {mu.detach().numpy().round(2)}\")\n",
    "print(f\"    - Log-Varianza (log_var): {log_var.detach().numpy().round(2)}\")\n",
    "\n",
    "# El vector latente z se muestrea usando la reparametrización.\n",
    "z = model.reparameterize(mu, log_var)\n",
    "print(f\"\\n1b. Se muestrea un vector latente 'z' de la nube: {z.detach().numpy().round(2)}\")\n",
    "\n",
    "print(f\"\\n1c. Decoder genera la reconstrucción X_hat a partir de 'z'.\")\n",
    "print(f\"    (Mostrando los primeros 10 de 35 píxeles de X_hat: {recon_X_hat.detach().numpy()[:10].round(2)})\")\n",
    "\n",
    "\n",
    "# --- PASO 2: Calcular Pérdida ---\n",
    "print(\"\\n--- [Paso 2] Calculando la Pérdida Dual ---\")\n",
    "loss_recon, loss_kl = loss_function(recon_X_hat, X, mu, log_var)\n",
    "total_loss = loss_recon + loss_kl\n",
    "\n",
    "print(f\"2a. Pérdida de Reconstrucción (qué tan diferente es X de X_hat): {loss_recon.item():.2f}\")\n",
    "print(f\"2b. Pérdida de Regularización KL (qué tan 'desorganizada' está la nube): {loss_kl.item():.2f}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"PÉRDIDA TOTAL (Suma de ambas): {total_loss.item():.2f}\")\n",
    "\n",
    "\n",
    "# --- PASO 3: Ajustar Parámetros ---\n",
    "print(\"\\n--- [Paso 3] Ajustando Parámetros del Modelo ---\")\n",
    "# Primero, veo un peso del modelo ANTES de la actualización.\n",
    "weight_antes = model.fc_output.weight.data[0, 0].item()\n",
    "print(f\"Peso de ejemplo ANTES de la actualización: {weight_antes:.4f}\")\n",
    "\n",
    "# Calculo los gradientes para todos los parámetros con respecto a la pérdida total.\n",
    "total_loss.backward()\n",
    "\n",
    "# El optimizador usa los gradientes para actualizar los pesos.\n",
    "optimizer.step()\n",
    "\n",
    "# Reviso el mismo peso DESPUÉS de la actualización.\n",
    "weight_despues = model.fc_output.weight.data[0, 0].item()\n",
    "print(f\"Peso de ejemplo DESPUÉS de la actualización: {weight_despues:.4f}\")\n",
    "print(\"¡El peso ha cambiado! El modelo está aprendiendo.\")\n",
    "\n",
    "\n",
    "# --- PASO 4: Y Repetir ---\n",
    "print(\"\\n--- [Paso 4] Repitiendo el Proceso ---\")\n",
    "print(\"Ahora, el ciclo completo se repetiría con nuevos datos (u los mismos).\")\n",
    "print(\"Veamos cómo cambia la pérdida si repetimos el proceso 5 veces más con el mismo dato:\")\n",
    "\n",
    "for i in range(5):\n",
    "    # Paso 1 y 2\n",
    "    recon_X_hat, mu, log_var = model(X)\n",
    "    loss_recon, loss_kl = loss_function(recon_X_hat, X, mu, log_var)\n",
    "    total_loss = loss_recon + loss_kl\n",
    "\n",
    "    # Paso 3\n",
    "    optimizer.zero_grad() # reseteo los gradientes antes de volver a calcularlos.\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"  Iteración {i+1}: Pérdida Total = {total_loss.item():.2f} (Recon: {loss_recon.item():.2f}, KL: {loss_kl.item():.2f})\")\n",
    "\n",
    "print(\"\\nComo se puede ver, la pérdida total tiende a disminuir a medida que el modelo se ajusta.\")\n",
    "print(\"--- FIN DE LA DEMOSTRACIÓN ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1192c1d",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7ec54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:25<00:00, 391kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 143kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.46MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando el entrenamiento de la GAN...\n",
      "[Época 1/25] [Pérdida D: 0.3028] [Pérdida G: 3.0886]\n",
      "[Época 2/25] [Pérdida D: 0.2147] [Pérdida G: 2.3709]\n",
      "[Época 3/25] [Pérdida D: 0.6653] [Pérdida G: 1.1661]\n",
      "[Época 4/25] [Pérdida D: 0.3138] [Pérdida G: 3.0648]\n",
      "[Época 5/25] [Pérdida D: 0.2672] [Pérdida G: 2.3449]\n",
      "[Época 6/25] [Pérdida D: 0.4606] [Pérdida G: 1.6277]\n",
      "[Época 7/25] [Pérdida D: 0.4339] [Pérdida G: 2.0970]\n",
      "[Época 8/25] [Pérdida D: 0.6758] [Pérdida G: 0.9634]\n",
      "[Época 9/25] [Pérdida D: 0.2383] [Pérdida G: 2.2362]\n",
      "[Época 10/25] [Pérdida D: 0.2985] [Pérdida G: 2.2868]\n",
      "[Época 11/25] [Pérdida D: 0.2392] [Pérdida G: 3.0469]\n",
      "[Época 12/25] [Pérdida D: 0.1661] [Pérdida G: 2.2942]\n",
      "[Época 13/25] [Pérdida D: 0.1168] [Pérdida G: 2.8628]\n",
      "[Época 14/25] [Pérdida D: 0.1212] [Pérdida G: 3.7772]\n",
      "[Época 15/25] [Pérdida D: 0.1394] [Pérdida G: 3.8864]\n",
      "[Época 16/25] [Pérdida D: 0.2688] [Pérdida G: 2.9559]\n",
      "[Época 17/25] [Pérdida D: 0.3876] [Pérdida G: 2.8976]\n",
      "[Época 18/25] [Pérdida D: 0.2652] [Pérdida G: 2.7580]\n",
      "[Época 19/25] [Pérdida D: 0.2605] [Pérdida G: 2.8542]\n",
      "[Época 20/25] [Pérdida D: 0.3039] [Pérdida G: 2.8973]\n",
      "[Época 21/25] [Pérdida D: 0.4974] [Pérdida G: 2.6100]\n",
      "[Época 22/25] [Pérdida D: 0.3067] [Pérdida G: 2.7548]\n",
      "[Época 23/25] [Pérdida D: 0.3138] [Pérdida G: 2.1926]\n",
      "[Época 24/25] [Pérdida D: 0.3028] [Pérdida G: 2.4082]\n",
      "[Época 25/25] [Pérdida D: 0.3775] [Pérdida G: 2.2616]\n",
      "Entrenamiento finalizado. Revisa la carpeta 'gan_basica_results' para ver las imágenes generadas.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# --- PASO 1: Mis Notas de Configuración ---\n",
    "# Defino algunos parámetros clave.\n",
    "latent_dim = 100      # Dimensión del ruido de entrada. Es como el tamaño del \"lienzo\" para el generador.\n",
    "batch_size = 64       # Cuántas imágenes procesar a la vez.\n",
    "epochs = 25           # Cuántas vueltas completas dar al conjunto de datos.\n",
    "learning_rate = 0.0002 # Qué tan grandes son los pasos que da el optimizador para aprender.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Usar la GPU si está disponible.\n",
    "\n",
    "# Me aseguro de tener una carpeta para guardar las imágenes que genere.\n",
    "# Así puedo ver cómo mi falsificador va mejorando con el tiempo.\n",
    "if not os.path.exists('gan_basica_results'):\n",
    "    os.makedirs('gan_basica_results')\n",
    "\n",
    "# --- PASO 2: Preparar los Datos Reales ---\n",
    "# Necesito los datos reales para enseñarle al detective cómo son los dígitos de verdad.\n",
    "# Voy a usar el famoso conjunto de datos MNIST.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convierto las imágenes a tensores de PyTorch.\n",
    "    transforms.Normalize([0.5], [0.5]) # Normalizo los píxeles al rango [-1, 1]. Esto es importante para que la función Tanh del generador funcione bien.\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset=mnist_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- PASO 3: Construir mis Redes Neuronales ---\n",
    "\n",
    "# Primero, el Falsificador (El Generador).\n",
    "# Su trabajo es tomar ruido aleatorio y convertirlo en algo que parezca un dígito.\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Mi generador va a ser una red neuronal simple, con capas lineales.\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 784), # 784 porque las imágenes son de 28x28 píxeles.\n",
    "            nn.Tanh() # La función Tanh asegura que la salida esté en el rango [-1, 1].\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Tomo el ruido 'z' y lo paso por el modelo.\n",
    "        img = self.model(z)\n",
    "        # Le doy la forma correcta a la salida (28x28).\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "# Ahora, el Detective (El Discriminador).\n",
    "# Su trabajo es tomar una imagen y dar una probabilidad de que sea real.\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() # La sigmoide da una probabilidad entre 0 (falsa) y 1 (real).\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Primero aplano la imagen para que entre en la red.\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# --- PASO 4: Poner Todo en Marcha ---\n",
    "# Creo instancias de mis dos redes.\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Defino la función de pérdida. La Entropía Cruzada Binaria es perfecta para este juego de clasificación.\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Creo optimizadores separados para cada red. Esto es clave.\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Iniciando el entrenamiento de la GAN...\")\n",
    "\n",
    "# --- PASO 5: El Ciclo de Entrenamiento ---\n",
    "# Aquí es donde ocurre la magia (y la batalla).\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_imgs, _) in enumerate(data_loader):\n",
    "\n",
    "        # Preparo las etiquetas objetivo: 1 para imágenes reales, 0 para falsas.\n",
    "        real_labels = torch.ones(real_imgs.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(real_imgs.size(0), 1).to(device)\n",
    "\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # --- Fase 1: Entrenar al Detective (Discriminador) ---\n",
    "        # El objetivo es que mejore en su trabajo de distinguir.\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Pérdida con imágenes reales: ¿qué tan bien las identifica como reales?\n",
    "        d_real_output = discriminator(real_imgs)\n",
    "        d_real_loss = loss_fn(d_real_output, real_labels)\n",
    "\n",
    "        # Genero un lote de imágenes falsas.\n",
    "        z_noise = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "        fake_imgs = generator(z_noise)\n",
    "\n",
    "        # Pérdida con imágenes falsas: ¿qué tan bien las identifica como falsas?\n",
    "        # Uso .detach() para que los gradientes no afecten al generador en este paso.\n",
    "        d_fake_output = discriminator(fake_imgs.detach())\n",
    "        d_fake_loss = loss_fn(d_fake_output, fake_labels)\n",
    "\n",
    "        # La pérdida total del detective es el promedio de ambas.\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --- Fase 2: Entrenar al Falsificador (Generador) ---\n",
    "        # El objetivo es que mejore engañando al detective.\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Genero un nuevo lote de imágenes falsas.\n",
    "        gen_imgs = generator(z_noise)\n",
    "\n",
    "        # Calculo la pérdida del generador. El generador GANA si el detective\n",
    "        # piensa que sus imágenes falsas son REALES (etiqueta 1).\n",
    "        g_loss = loss_fn(discriminator(gen_imgs), real_labels)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # Imprimo el progreso al final de cada época para ver cómo van las pérdidas.\n",
    "    print(f\"[Época {epoch+1}/{epochs}] [Pérdida D: {d_loss.item():.4f}] [Pérdida G: {g_loss.item():.4f}]\")\n",
    "\n",
    "    # Guardo una muestra de las imágenes generadas en esta época para ver el progreso visualmente.\n",
    "    save_image(gen_imgs.data[:25], f\"gan_basica_results/epoch_{epoch+1}.png\", nrow=5, normalize=True)\n",
    "\n",
    "print(\"Entrenamiento finalizado. Revisa la carpeta 'gan_basica_results' para ver las imágenes generadas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
