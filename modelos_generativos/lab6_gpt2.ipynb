{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9edc03",
   "metadata": {},
   "source": [
    "## Huggin Face\n",
    "Repositorio de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99293d81",
   "metadata": {},
   "source": [
    "## Importar libreria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684eee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando modelo ...\n",
      "Modelo cargado\n"
     ]
    }
   ],
   "source": [
    "modelo = 'gpt2-large'\n",
    "\n",
    "# Descargar modelo\n",
    "AutoTokenizer.from_pretrained(modelo)\n",
    "\n",
    "print(\"Descargando modelo ...\")\n",
    "\n",
    "# Cargar el modelo\n",
    "AutoModel.from_pretrained(modelo)\n",
    "print(\"Modelo cargado\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d232c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Cargado a RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La intelilgencia artificial esse del nombre de la perezada por el de la una perezada de los de los de los las alcalde.\n",
      "\n",
      "La perezada de los de las alcalde en la la perezada.\n",
      "\n",
      "El perezada de los de las alcalde en la perezada de los de los de los.\n",
      "\n",
      "El perezada por el sua perezada.\n",
      "\n",
      "El perezada por el sua perezada.\n",
      "\n",
      "El perezada por el sua perezada por el sua sua.\n",
      "\n",
      "El perezada por el sua perezada por el sua sua por el sua.\n",
      "\n",
      "Etodo los de los de las alcalde en la perezada por el sua perezada por el sua por el sua.\n",
      "\n",
      "Etodo los de los de las alcalde en la perezada por el sua perezada por el sua por el sua.\n",
      "\n",
      "El perezada por el sua perezada por el sua perezada\n",
      "La intelilgencia artificial ese, ella giorno di ciudad del mezzo di mezzo. Il nostra ciudad il di parede di mezzo, il di ciudad il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di parede il mezzo, il di pared\n",
      "La intelilgencia artificial esculentes, est anciencias otraque de mejor los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuerdos con esse una creo, el más de los más, que no acuer\n"
     ]
    }
   ],
   "source": [
    "# Cargando el modelo en RAM\n",
    "generador = pipeline('text-generation', model=modelo)\n",
    "\n",
    "print(\"Modelo Cargado a RAM\")\n",
    "\n",
    "\n",
    "prompt = \"La intelilgencia artificial es\"\n",
    "\n",
    "resultado = generador(prompt, max_length=50, num_return_sequences=3)\n",
    "\n",
    "for r in resultado:\n",
    "    print(r[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9079f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
